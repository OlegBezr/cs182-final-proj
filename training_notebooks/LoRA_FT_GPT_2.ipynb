{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XgivtTZp6Au"
      },
      "source": [
        "Set up basics:\n",
        "1. Keras Library\n",
        "2. Mixed precision training to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy_GLSYsp6Av",
        "outputId": "24a5ada6-460d-41c2-ff29-1f926f0fa2d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/584.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/584.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.5/584.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-nlp -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gm4SKqY4xA_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeZsC7gap6Av",
        "outputId": "5257483b-17cc-47be-eb52-734b0fe86f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        }
      ],
      "source": [
        "import keras_nlp\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
        "keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFRW5HGWp6Av"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckwFvdkCp6Ax"
      },
      "outputs": [],
      "source": [
        "# General hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_BATCHES = 500\n",
        "EPOCHS = 1  # Can be set to a higher value for better results\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "MAX_GENERATION_LENGTH = 200\n",
        "\n",
        "GPT2_PRESET = \"gpt2_base_en\"\n",
        "\n",
        "# LoRA-specific hyperparameters\n",
        "RANK = 4\n",
        "ALPHA = 32.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LUMotrtp6Ax"
      },
      "source": [
        "\n",
        "Load in the left, right, neutral dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htbfqV-jp6Ax",
        "outputId": "0f9a78d0-de04-44c7-f15c-516a983f9d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the local path to the project"
      ],
      "metadata": {
        "id": "PUdS3urP03XE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m5qSqsF4OPl",
        "outputId": "ad58400d-26f8-4722-c9fb-6ed32d2ccea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1fHTJEwQpdw9d5-ck7bXjejartUco15Ex/cs182-final-proj\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/cs182-final-proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6mdxYfvbAEV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Read CSV and process the data\n",
        "df = pd.read_csv(\"combined_data.csv\")\n",
        "df = df.dropna()\n",
        "\n",
        "# Create lists to store the articles\n",
        "left_articles = []\n",
        "right_articles = []\n",
        "neu_articles = []\n",
        "\n",
        "# Iterate through the dataframe and append articles to the lists\n",
        "for idx, item in df.iterrows():\n",
        "    article = item[\"text\"]\n",
        "    if item['type'] == 'center':\n",
        "        neu_articles.append(article)\n",
        "    elif item['type'] == 'left':\n",
        "        left_articles.append(article)\n",
        "    elif item['type'] == 'right':\n",
        "        right_articles.append(article)\n",
        "\n",
        "# Convert lists to tf.data.Dataset objects\n",
        "left_data = tf.data.Dataset.from_tensor_slices(left_articles)\n",
        "right_data = tf.data.Dataset.from_tensor_slices(right_articles)\n",
        "neu_data = tf.data.Dataset.from_tensor_slices(neu_articles)\n",
        "\n",
        "# Now you can apply the dataset operations\n",
        "BATCH_SIZE = 32  # Set your batch size\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCeHO1Nbp6Ay"
      },
      "source": [
        "As a test, the model is only trained on batched left_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-gaQ_A0p6Ay"
      },
      "outputs": [],
      "source": [
        "train_ds = (\n",
        "    left_data\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "train_ds = train_ds.take(NUM_BATCHES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OmsZZmnp6Ay"
      },
      "source": [
        "Text generation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuGwTjw8p6Ay"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_text(model, input_text, max_length=200):\n",
        "    start = time.time()\n",
        "\n",
        "    output = model.generate(input_text, max_length=max_length)\n",
        "    print(\"\\nOutput:\")\n",
        "    print(output)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Total Time Elapsed: {end - start:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJKH8rngp6Ay"
      },
      "source": [
        "\n",
        "\n",
        "We will use AdamW optimizer and cross-entropy loss for training both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcpGkXiRp6Ay"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_optimizer_and_loss():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        epsilon=1e-6,\n",
        "        global_clipnorm=1.0,  # Gradient clipping.\n",
        "    )\n",
        "    # Exclude layernorm and bias terms from weight decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return optimizer, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "jLX_XI4Wp6Ay",
        "outputId": "2173be43-52f6-4dbe-d42a-a98ffb6a4eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/vocab.json\n",
            "1042301/1042301 [==============================] - 1s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/merges.txt\n",
            "456318/456318 [==============================] - 1s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/model.h5\n",
            "497986112/497986112 [==============================] - 23s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                     │                                              \u001b[38;5;34m50,257\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                      │     \u001b[38;5;34m124,439,808\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)                    │      \u001b[38;5;34m38,597,376\u001b[0m │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)                    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")\n",
        "\n",
        "gpt2_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHxkG-Cxp6Ay"
      },
      "source": [
        "Training and text generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj5M4Qj0p6Ay",
        "outputId": "de1e034f-c2a6-4c21-cb11-89ee5e1b94d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 591s 22s/step - loss: 1.2995 - accuracy: 0.3374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dee446692a0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "gpt2_lm.fit(train_ds, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBm4gdU1p6Ay",
        "outputId": "01e447ab-31f0-4440-c7d8-0da086afd11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "Trump, who has said he is committed to a \"total and complete shutdown of Muslims entering the United States until our country's representatives can figure out what is going on,\" is now trying to make a political case to the American people that the Muslim ban is not an attack.\n",
            "Total Time Elapsed: 35.19s\n"
          ]
        }
      ],
      "source": [
        "generate_text(gpt2_lm, \"Trump\", max_length=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLGxhfJWp6Az"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class LoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        rank=8,\n",
        "        alpha=32,\n",
        "        trainable=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # We want to keep the name of this layer the same as the original\n",
        "        # dense layer.\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
        "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
        "\n",
        "        # Layers.\n",
        "\n",
        "        # Original dense layer.\n",
        "        self.original_layer = original_layer\n",
        "        # No matter whether we are training the model or are in inference mode,\n",
        "        # this layer should be frozen.\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "        # LoRA dense layers.\n",
        "        self.A = keras.layers.Dense(\n",
        "            units=rank,\n",
        "            use_bias=False,\n",
        "            # Note: the original paper mentions that normal distribution was\n",
        "            # used for initialization. However, the official LoRA implementation\n",
        "            # uses \"Kaiming/He Initialization\".\n",
        "            kernel_initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_A\",\n",
        "        )\n",
        "        # B has the same `equation` and `output_shape` as the original layer.\n",
        "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
        "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
        "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
        "        # `c` represents `rank`.\n",
        "        self.B = keras.layers.EinsumDense(\n",
        "            equation=original_layer_config[\"equation\"],\n",
        "            output_shape=original_layer_config[\"output_shape\"],\n",
        "            kernel_initializer=\"zeros\",\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_B\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "        if self.trainable:\n",
        "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
        "            # to the original layer's output.\n",
        "            lora_output = self.B(self.A(inputs)) * self._scale\n",
        "            return original_output + lora_output\n",
        "\n",
        "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
        "        # the original layer's weights - more on this in the text generation\n",
        "        # section!\n",
        "        return original_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiBUk7afp6Az"
      },
      "source": [
        "Insert LoRA layer into the old model:\n",
        "\n",
        "- Delete previous model;\n",
        "- Load a new GPT-2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txNwRUGep6Az"
      },
      "outputs": [],
      "source": [
        "del gpt2_lm\n",
        "del optimizer\n",
        "del loss\n",
        "\n",
        "# Load the original model.\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "lora_model = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2879FYxvp6Az"
      },
      "source": [
        "Override the original query/value projection matrices with\n",
        "new LoRA layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnSuq4oBp6Az"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    # Change query dense layer.\n",
        "    decoder_layer = lora_model.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
        "    self_attention_layer = decoder_layer._self_attention_layer\n",
        "\n",
        "    # Change query dense layer.\n",
        "    self_attention_layer._query_dense = LoraLayer(\n",
        "        self_attention_layer._query_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "    # Change value dense layer.\n",
        "    self_attention_layer._value_dense = LoraLayer(\n",
        "        self_attention_layer._value_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-MzjB0zp6Az"
      },
      "source": [
        "Freeze the entire LLM, only the LoRA layers should be trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex_PfutKp6Az"
      },
      "outputs": [],
      "source": [
        "for layer in lora_model._flatten_layers():\n",
        "    lst_of_sublayers = list(layer._flatten_layers())\n",
        "\n",
        "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
        "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6pcf_4Wp6Az"
      },
      "source": [
        "Print the model's summary and see if the number of non-trainable parameters and\n",
        "total parameters are correct.\n",
        "\n",
        "In a previous section, we had calculated the number of parameters associated with\n",
        "the LoRA layers to be 6,144. The total trainable parameters in the model should\n",
        "be `num_layers * (query, value) * 6,144 = 12 * 2 * 6,144 = 147,456`. The\n",
        "number of non-trainable parameters should be the same as the total number of\n",
        "parameters in the original GPT-2 model, which is `124,439,808`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "bQV44uQMp6A2",
        "outputId": "63a6b382-297e-49ce-ecfd-268ebcb0ea20"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer_1 (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                   │                                              \u001b[38;5;34m50,257\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                   │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt2_causal_lm_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone_1 (\u001b[38;5;33mGPT2Backbone\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                      │     \u001b[38;5;34m124,587,264\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)                    │      \u001b[38;5;34m38,597,376\u001b[0m │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,587,264</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)                    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,587,264\u001b[0m (475.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,587,264</span> (475.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m147,456\u001b[0m (576.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span> (576.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "lora_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AROJdomUp6A2",
        "outputId": "b1c0a536-3795-4b6c-b586-bb6c5051de19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 321s 12s/step - loss: 1.4104 - accuracy: 0.3018\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dee2fc2bc40>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "optimizer, loss = get_optimizer_and_loss()\n",
        "\n",
        "lora_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "lora_model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsojVmGIp6A2"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    self_attention_layer = lora_model.backbone.get_layer(\n",
        "        f\"transformer_layer_{layer_idx}\"\n",
        "    )._self_attention_layer\n",
        "\n",
        "    # Merge query dense layer.\n",
        "    query_lora_layer = self_attention_layer._query_dense\n",
        "\n",
        "    A_weights = query_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = query_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    query_lora_layer.original_layer.kernel.assign_add(increment_weights)\n",
        "\n",
        "    # Merge value dense layer.\n",
        "    value_lora_layer = self_attention_layer._value_dense\n",
        "\n",
        "    A_weights = value_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = value_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    value_lora_layer.original_layer.kernel.assign_add(increment_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr3wWWmIp6A2"
      },
      "source": [
        "Text generation with LoRA:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\n",
        "    lora_model, \"Trump 2024\", max_length=MAX_GENERATION_LENGTH\n",
        ")\n",
        "\n",
        "generate_text(\n",
        "    lora_model, \"Trump 2024\", max_length=MAX_GENERATION_LENGTH\n",
        ")\n",
        "\n",
        "generate_text(\n",
        "    lora_model, \"Trump 2024\", max_length=MAX_GENERATION_LENGTH\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Yt_5thdcQE",
        "outputId": "a74d8ab0-7e20-408c-e54a-d3ea84073f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "Trump 2024 campaign: 'We're going to build it' — as Trump says\n",
            "\n",
            "The president-elect is now taking a more aggressive stance on infrastructure, calling on the federal government to invest in infrastructure to help pay for infrastructure.\n",
            "\n",
            "Trump, who has been criticized for failing to deliver on his campaign promises, has pledged to invest at least $100 billion in infrastructure in the next 10 years, including $10 billion on highways, bridges and airports, as well as $5 billion in new public transit and $2 billion to build new airports.\n",
            "\n",
            "The president-elect's campaign has been trying to drum up support for infrastructure investments for years.\n",
            "\n",
            "Trump recently said he wants to build a wall along the southern border with Mexico, but he has yet to announce any plans for that wall.\n",
            "\n",
            "The White House has said it would build a wall along the southern border with Mexico, but has yet to announce any plans for the wall.\n",
            "\n",
            "On Monday, the president\n",
            "Total Time Elapsed: 71.76s\n",
            "\n",
            "Output:\n",
            "Trump 2024 Olympics: What's next?\n",
            "\n",
            "Trump's Olympics bid has already been met in a series of setbacks, most notably with a $2 billion loan from the International Monetary Fund.\n",
            "\n",
            "The IOC, which has been under pressure from the Trump administration for a host of issues, said in a statement that the IOC had not yet made its decision.\n",
            "\n",
            "The Trump Olympics is a \"major disappointment\" for the IOC, said Olympic Committee President Thomas Bach, a former U.S. attorney and a member of the Trump administration, and a \"disappointing failure.\"\n",
            "\n",
            "In addition to the Olympics, Trump has pledged that he would \"make the United States one of the most competitive nations on earth\" if elected, Bach said.\n",
            "\n",
            "Trump's bid for Olympic gold is also in jeopardy because of the lack of funding, which has led to the IOC and the Trump Organization's decision to withdraw from the race in July after a federal judge refused to reinstate the\n",
            "Total Time Elapsed: 52.79s\n",
            "\n",
            "Output:\n",
            "Trump 2024 campaign slogan: \"Make America Great Again.\" (Photo: AP) Story Highlights Trump's campaign said the campaign was looking for new ways to raise money\n",
            "\n",
            "Trump has said he is looking to build a \"massive, massive wall\" along the U.S.-Mexico border\n",
            "\n",
            "The campaign has been working for months\n",
            "\n",
            "WASHINGTON (AP) — President Donald Trump's campaign is working to raise $1 billion for the 2020 presidential election, a campaign aide said Tuesday.\n",
            "\n",
            "Trump's campaign spokesman, Jason Miller, said the campaign has been working to raise $1 billion for the 2020 presidential election, a campaign aide said Tuesday. The campaign said Trump is seeking $1.2 billion in new funding.\n",
            "\n",
            "Trump has said he is looking to build a massive, massive wall along the U.S.-Mexico border and build a wall on the U.S.-Mexican border. He's also said he's looking into building a wall on his home in the White\n",
            "Total Time Elapsed: 52.07s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}